# Engram

> Plug-and-play memory engine for AI agents â€” one import, any LLM, any storage, and your agent never forgets.

**Version**: 0.1.0 (Phase 1)
**License**: MIT

## Why Engram?

Every agentic framework re-invents memory. Engram is different:

- **Zero infrastructure** â€” Default store is a single JSON file. No database required.
- **BYOLLM** â€” You pass an `async (prompt) => string` function. We never import a provider.
- **BYOE** â€” Embeddings are optional. Everything works (worse) without them.
- **Small core** â€” Core engine is under 500 lines. Everything else is adapters.
- **Surprise-first** â€” Novel information is retained. Redundant information is rejected. No LLM call for this decision.
- **Export-native** â€” Memories are always exportable to JSON, Markdown, CSV. Never locked in.
- **Transparent** â€” Every scoring decision is explainable. Every store/reject has a reason.

## Installation

```bash
npm install engram
```

## Quick Start

### With Claude (Anthropic)

```typescript
import Anthropic from '@anthropic-ai/sdk';
import { Engram } from 'engram';

const client = new Anthropic();

const mem = new Engram({
  llm: async (prompt) => {
    const r = await client.messages.create({
      model: 'claude-sonnet-4-5-20250929',
      max_tokens: 1024,
      messages: [{ role: 'user', content: prompt }],
    });
    return r.content[0].text;
  },
});

// Extract and store memories from a conversation
const result = await mem.remember([
  { role: 'user', content: 'I prefer TypeScript over JavaScript' },
  { role: 'assistant', content: 'Got it! I'll use TypeScript for your projects.' },
]);

console.log(`Stored ${result.stored.length} memories`);
console.log(`Rejected ${result.rejected.length} (too redundant)`);

// Recall relevant memories
const relevant = await mem.recall('what language does the user prefer?');
console.log(relevant[0].content); // "User prefers TypeScript over JavaScript"
```

### With OpenAI

```typescript
import OpenAI from 'openai';
import { Engram } from 'engram';

const openai = new OpenAI();

const mem = new Engram({
  llm: async (p) => {
    const r = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: p }],
    });
    return r.choices[0].message.content;
  },
  embed: async (text) => {
    const r = await openai.embeddings.create({
      model: 'text-embedding-3-small',
      input: text,
    });
    return r.data[0].embedding;
  },
});
```

### With Ollama (free, local)

```typescript
import { Engram } from 'engram';

const mem = new Engram({
  llm: async (prompt) => {
    const r = await fetch('http://localhost:11434/api/generate', {
      method: 'POST',
      body: JSON.stringify({ model: 'qwen2.5:7b', prompt, stream: false }),
    });
    return (await r.json()).response;
  },
  embed: async (text) => {
    const r = await fetch('http://localhost:11434/api/embeddings', {
      method: 'POST',
      body: JSON.stringify({ model: 'nomic-embed-text', prompt: text }),
    });
    return (await r.json()).embedding;
  },
});
```

### Manual mode (no LLM, no embeddings)

```typescript
import { Engram, JsonFileStore } from 'engram';

const mem = new Engram({ store: new JsonFileStore('./brain.json') });

await mem.store('User prefers TypeScript', { category: 'preference' });
await mem.store('Project uses Next.js 15', { category: 'context' });

const relevant = await mem.recall('what framework');
// Returns memories matching "framework" by keyword similarity
```

## Core API

### `remember(messages, opts?)` â€” Extract memories from conversation

```typescript
const result = await mem.remember([
  { role: 'user', content: '...' },
  { role: 'assistant', content: '...' },
], {
  source: 'session-001',    // Track where memories came from
  dryRun: true,             // Preview without writing
  explain: true,            // Include scoring breakdown
});

console.log(result.stored);   // Memories that were stored
console.log(result.rejected); // Memories that were too redundant
console.log(result.errors);   // Any LLM errors
```

### `store(content, opts?)` â€” Manually store a memory

```typescript
const memory = await mem.store('Deploy with `vercel --prod`', {
  category: 'skill',
  importance: 0.9,
  metadata: { project: 'my-app' },
});
```

### `recall(query, opts?)` â€” Retrieve memories

```typescript
const memories = await mem.recall('how to deploy', {
  k: 5,                           // Top 5 results
  categories: ['skill'],          // Filter by category
  minImportance: 0.5,             // After decay
  explain: true,                  // Show scoring
});
```

### `forget(opts?)` â€” Prune old memories

```typescript
const result = await mem.forget({
  mode: 'normal',  // 'gentle' | 'normal' | 'aggressive'
  dryRun: true,    // Preview what would be deleted
});

console.log(`Would prune ${result.pruned} memories`);
```

### `export(format)` and `import(data, format)`

```typescript
// Export
const json = await mem.export('json');
const markdown = await mem.export('md');
const csv = await mem.export('csv');

// Import
const count = await mem.import(json, 'json');
```

### `stats()` â€” Get statistics

```typescript
const stats = await mem.stats();
console.log(stats.totalMemories);
console.log(stats.byCategory);
console.log(stats.averageImportance);
```

## Configuration

```typescript
const mem = new Engram({
  // Adapters
  llm: myLlmFunction,              // Optional â€” needed for remember()
  embed: myEmbedFunction,          // Optional â€” better recall with it
  store: new JsonFileStore(),      // Default: JsonFileStore('./engram.json')

  // Scoring
  surpriseThreshold: 0.3,          // Min novelty to store (0-1)
  importanceBoost: {               // Category multipliers
    fact: 1.0,
    preference: 1.2,
    skill: 1.3,
    episode: 0.8,
    context: 0.9,
  },

  // Categories
  categories: ['fact', 'preference', 'skill', 'episode', 'context'],

  // Decay
  decayHalfLifeDays: 30,           // Importance halves every 30 days
  maxRetentionDays: 90,            // Hard expiry
  maxMemories: 10_000,             // Max total memories

  // Retrieval
  defaultK: 5,                     // Default results per recall

  // Namespace
  namespace: 'default',            // Isolate memory pools

  // Versioning
  trackHistory: true,              // Track content changes
  maxHistoryPerMemory: 10,         // Max versions kept
});
```

## Events

```typescript
mem.on('stored', (memory) => {
  console.log(`ðŸ“ Stored: "${memory.content}" [${memory.category}]`);
});

mem.on('rejected', (info) => {
  console.log(`â­ï¸  Skipped: "${info.content}" â€” ${info.reason}`);
});

mem.on('recalled', (memories, query) => {
  console.log(`ðŸ” Recalled ${memories.length} for "${query}"`);
});

mem.on('forgotten', (ids, count) => {
  console.log(`ðŸ—‘ï¸  Pruned ${count} memories`);
});

mem.on('error', (err) => {
  console.error(`âŒ Error: ${err.message}`);
});
```

## Hooks

```typescript
const mem = new Engram({
  hooks: {
    // Redact PII before storing
    beforeStore: (memory) => {
      memory.content = memory.content.replace(/\b[\w.-]+@[\w.-]+\.\w+\b/g, '[EMAIL]');
      return memory;
    },

    // Expand queries
    beforeRecall: (query) => {
      return `${query} (project: my-app)`;
    },

    // Archive before deleting
    beforeForget: async (memories) => {
      await archiveToS3(memories);
      return memories;
    },
  },
});
```

## Explainability

Every decision can be inspected:

```typescript
// See why memories were stored or rejected
const result = await mem.remember(messages, { explain: true });
console.log(result.stored[0].explanation);
// "surprise: 0.721 (semantic: 0.683, keyword: 0.812, rarity: 0.450)
//  Ã— category_boost(preference): 1.2 â†’ importance: 0.865
//  â†’ STORED (above threshold 0.3)"

// See why memories were recalled
const memories = await mem.recall('deploy', { explain: true });
console.log(memories[0].explanation);
// "retrieval_score: 0.823
//    relevance: 0.912 (cosine similarity with query)
//    importance: 0.760 (base: 0.920, decayed from 30d, access_boost: 1.1x)
//    recency: 0.650 (30 days old)"
```

## Store Adapters

### Built-in

- **MemoryStore** â€” In-memory (no persistence, great for testing)
- **JsonFileStore** â€” Single JSON file (default, zero setup)

### Coming in Phase 2+

- **SqliteStore** â€” Local SQLite database
- Community adapters: PostgreSQL, Redis, Qdrant, Turso

## Roadmap

- âœ… **Phase 1** (v0.1.0) â€” Core memory engine with JSON/Markdown export
- ðŸš§ **Phase 2** (v0.2.0) â€” Smart recall, context injection, SqliteStore
- ðŸ“… **Phase 3** (v0.3.0) â€” Multi-agent, namespaces, shared memory
- ðŸ“… **Phase 4** (v0.4.0) â€” Tool use, MCP server, skill learning
- ðŸ“… **Phase 5** (v1.0.0) â€” Production hardening, Python SDK, analytics

## Philosophy

Engram follows seven non-negotiable principles:

1. **Zero infrastructure** â€” No database required to start
2. **BYOLLM** â€” Never import a provider, you pass the function
3. **BYOE** â€” Embeddings optional, everything works without them
4. **Small core** â€” Core under 500 lines, rest is adapters
5. **Surprise-first** â€” Novel info retained, redundant rejected
6. **Export-native** â€” Never locked in, always exportable
7. **Transparent** â€” Every decision is explainable

## Contributing

This is Phase 1. We welcome contributions for:

- Store adapters (SQLite, PostgreSQL, Redis, etc.)
- Embedding providers (Voyage, Cohere, etc.)
- Bug fixes and tests
- Documentation improvements

## License

MIT Â© Engram Contributors

---

**Built with inspiration from:** Titans (Google), Mem0, Letta, Lynkr, and the broader agentic memory research community.
